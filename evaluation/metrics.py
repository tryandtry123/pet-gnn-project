"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
"""

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    precision_recall_curve, roc_curve, average_precision_score
)
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# å¯¼å…¥å­—ä½“å·¥å…·
import sys
sys.path.append('..')
try:
    from utils.plot_utils import setup_chinese_font, get_chinese_labels, save_figure_with_chinese_support
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è®¾ç½®
    def setup_chinese_font():
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            return True
        except:
            return False
    
    def get_chinese_labels():
        return {
            'title_confusion': 'PET Low-Coupling Event Recognition - Confusion Matrix',
            'title_roc': 'PET Low-Coupling Event Recognition - ROC Curve', 
            'title_pr': 'PET Low-Coupling Event Recognition - PR Curve',
            'xlabel_confusion': 'Predicted Label',
            'ylabel_confusion': 'True Label',
            'xlabel_roc': 'False Positive Rate (FPR)',
            'ylabel_roc': 'True Positive Rate (TPR)',
            'xlabel_pr': 'Recall',
            'ylabel_pr': 'Precision',
            'class_names': ['Valid Event', 'Low-Coupling Event'],
            'legend_roc': 'ROC Curve (AUC = {:.3f})',
            'legend_pr': 'PR Curve (AP = {:.3f})',
            'text_accuracy': 'Accuracy: {:.3f}',
            'text_precision': 'Precision: {:.3f}',
            'text_recall': 'Recall: {:.3f}',
            'text_f1': 'F1-Score: {:.3f}'
        }
    
    def save_figure_with_chinese_support(fig, filepath, dpi=300, bbox_inches='tight'):
        fig.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches, facecolor='white')

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_font()

def calculate_metrics(y_true, y_pred, y_prob=None, class_names=None):
    """
    è®¡ç®—å®Œæ•´çš„åˆ†ç±»æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾  
        y_prob: é¢„æµ‹æ¦‚ç‡ (å¯é€‰)
        class_names: ç±»åˆ«åç§° (å¯é€‰)
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
    """
    # è·å–æ ‡ç­¾é…ç½®
    labels = get_chinese_labels()
    if class_names is None:
        class_names = labels['class_names']
    
    # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),
        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),
        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),
        'precision_weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),
        'recall_weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),
        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),
    }
    
    # å„ç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)
    
    for i, class_name in enumerate(class_names):
        metrics[f'precision_{class_name}'] = precision_per_class[i] if i < len(precision_per_class) else 0
        metrics[f'recall_{class_name}'] = recall_per_class[i] if i < len(recall_per_class) else 0
        metrics[f'f1_{class_name}'] = f1_per_class[i] if i < len(f1_per_class) else 0
    
    # å¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼Œè®¡ç®—AUCç­‰æŒ‡æ ‡
    if y_prob is not None:
        try:
            if len(np.unique(y_true)) == 2:  # äºŒåˆ†ç±»
                metrics['roc_auc'] = roc_auc_score(y_true, y_prob[:, 1] if y_prob.ndim > 1 else y_prob)
                metrics['ap_score'] = average_precision_score(y_true, y_prob[:, 1] if y_prob.ndim > 1 else y_prob)
            else:  # å¤šåˆ†ç±»
                metrics['roc_auc_ovr'] = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')
                metrics['roc_auc_ovo'] = roc_auc_score(y_true, y_prob, multi_class='ovo', average='macro')
        except Exception as e:
            print(f"AUCè®¡ç®—å¤±è´¥: {e}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    metrics['confusion_matrix'] = cm
    
    return metrics


def print_classification_report(y_true, y_pred, class_names=None, save_path=None):
    """
    æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        class_names: ç±»åˆ«åç§°
        save_path: ä¿å­˜è·¯å¾„ (å¯é€‰)
    """
    if class_names is None:
        class_names = ['æœ‰æ•ˆäº‹ä»¶', 'ä½è€¦åˆäº‹ä»¶']
    
    print("=" * 60)
    print("ğŸ“Š PET-GNNæ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(y_true, y_pred, class_names=class_names)
    
    # åŸºç¡€æŒ‡æ ‡
    print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½:")
    print(f"  å‡†ç¡®ç‡ (Accuracy):     {metrics['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡ (Precision):    {metrics['precision_weighted']:.4f}")
    print(f"  å¬å›ç‡ (Recall):       {metrics['recall_weighted']:.4f}")
    print(f"  F1åˆ†æ•° (F1-Score):     {metrics['f1_weighted']:.4f}")
    
    # å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    print(f"\nğŸ“‹ å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
    for class_name in class_names:
        print(f"\n  {class_name}:")
        print(f"    ç²¾ç¡®ç‡: {metrics.get(f'precision_{class_name}', 0):.4f}")
        print(f"    å¬å›ç‡: {metrics.get(f'recall_{class_name}', 0):.4f}")
        print(f"    F1åˆ†æ•°: {metrics.get(f'f1_{class_name}', 0):.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = metrics['confusion_matrix']
    print(f"\nğŸ” æ··æ·†çŸ©é˜µ:")
    print("     é¢„æµ‹")
    print("çœŸå®", end="  ")
    for name in class_names:
        print(f"{name:>8}", end="")
    print()
    
    for i, true_name in enumerate(class_names):
        print(f"{true_name:>4}", end="  ")
        for j in range(len(class_names)):
            print(f"{cm[i,j]:>8}", end="")
        print()
    
    # é”™è¯¯åˆ†æ
    print(f"\nâš ï¸ é”™è¯¯åˆ†æ:")
    total_samples = len(y_true)
    correct_samples = np.sum(y_true == y_pred)
    wrong_samples = total_samples - correct_samples
    
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  æ­£ç¡®é¢„æµ‹: {correct_samples} ({correct_samples/total_samples*100:.2f}%)")
    print(f"  é”™è¯¯é¢„æµ‹: {wrong_samples} ({wrong_samples/total_samples*100:.2f}%)")
    
    # ç±»åˆ«åˆ†å¸ƒ
    print(f"\nğŸ“ˆ çœŸå®æ ‡ç­¾åˆ†å¸ƒ:")
    unique, counts = np.unique(y_true, return_counts=True)
    for i, (label, count) in enumerate(zip(unique, counts)):
        class_name = class_names[label] if label < len(class_names) else f"ç±»åˆ«{label}"
        print(f"  {class_name}: {count} ({count/total_samples*100:.2f}%)")
    
    # ä¿å­˜æŠ¥å‘Š
    if save_path:
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(classification_report(y_true, y_pred, target_names=class_names))
            print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    print("=" * 60)
    
    return metrics


def plot_roc_curve(y_true, y_prob, save_path=None, figsize=(8, 6)):
    """ç»˜åˆ¶ROCæ›²çº¿"""
    try:
        # è·å–æ ‡ç­¾é…ç½®
        labels = get_chinese_labels()
        
        plt.figure(figsize=figsize)
        
        if y_prob.ndim > 1:
            # å¤šåˆ†ç±»æƒ…å†µï¼Œåªå¤„ç†ç¬¬äºŒåˆ—ï¼ˆä½è€¦åˆäº‹ä»¶æ¦‚ç‡ï¼‰
            prob_positive = y_prob[:, 1]
        else:
            prob_positive = y_prob
            
        # è®¡ç®—ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(y_true, prob_positive)
        auc = roc_auc_score(y_true, prob_positive)
        
        # ç»˜åˆ¶æ›²çº¿
        plt.plot(fpr, tpr, linewidth=2, label=labels['legend_roc'].format(auc))
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        plt.xlabel(labels['xlabel_roc'], fontsize=12)
        plt.ylabel(labels['ylabel_roc'], fontsize=12)
        plt.title(labels['title_roc'], fontsize=14, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ€§èƒ½æ–‡æœ¬
        plt.text(0.6, 0.2, f"AUC = {auc:.3f}", fontsize=12, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
        
        plt.tight_layout()
        
        if save_path:
            save_figure_with_chinese_support(plt.gcf(), save_path)
        
        return auc
        
    except Exception as e:
        print(f"ROCæ›²çº¿ç»˜åˆ¶å¤±è´¥: {e}")
        return None


def plot_precision_recall_curve(y_true, y_prob, save_path=None, figsize=(8, 6)):
    """ç»˜åˆ¶Precision-Recallæ›²çº¿"""
    try:
        # è·å–æ ‡ç­¾é…ç½®
        labels = get_chinese_labels()
        
        plt.figure(figsize=figsize)
        
        if y_prob.ndim > 1:
            prob_positive = y_prob[:, 1]
        else:
            prob_positive = y_prob
            
        # è®¡ç®—PRæ›²çº¿
        precision, recall, _ = precision_recall_curve(y_true, prob_positive)
        ap = average_precision_score(y_true, prob_positive)
        
        # ç»˜åˆ¶æ›²çº¿
        plt.plot(recall, precision, linewidth=2, label=labels['legend_pr'].format(ap))
        
        # åŸºå‡†çº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰
        no_skill = len(y_true[y_true==1]) / len(y_true)
        plt.axhline(y=no_skill, color='k', linestyle='--', alpha=0.5, 
                   label=f'Random Classifier (AP = {no_skill:.3f})')
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        plt.xlabel(labels['xlabel_pr'], fontsize=12)
        plt.ylabel(labels['ylabel_pr'], fontsize=12)
        plt.title(labels['title_pr'], fontsize=14, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ€§èƒ½æ–‡æœ¬
        plt.text(0.2, 0.8, f"AP = {ap:.3f}", fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
        
        plt.tight_layout()
        
        if save_path:
            save_figure_with_chinese_support(plt.gcf(), save_path)
        
        return ap
        
    except Exception as e:
        print(f"PRæ›²çº¿ç»˜åˆ¶å¤±è´¥: {e}")
        return None


def analyze_prediction_errors(y_true, y_pred, X=None, feature_names=None, max_errors=10):
    """
    åˆ†æé¢„æµ‹é”™è¯¯çš„æ ·æœ¬
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        X: ç‰¹å¾çŸ©é˜µ (å¯é€‰)
        feature_names: ç‰¹å¾åç§° (å¯é€‰)
        max_errors: æ˜¾ç¤ºçš„æœ€å¤§é”™è¯¯æ•°é‡
    """
    print("\nğŸ” é”™è¯¯æ ·æœ¬åˆ†æ:")
    print("-" * 40)
    
    # æ‰¾å‡ºé”™è¯¯é¢„æµ‹çš„æ ·æœ¬
    error_mask = y_true != y_pred
    error_indices = np.where(error_mask)[0]
    
    if len(error_indices) == 0:
        print("ğŸ‰ æ²¡æœ‰é¢„æµ‹é”™è¯¯çš„æ ·æœ¬!")
        return
    
    print(f"æ€»é”™è¯¯æ•°é‡: {len(error_indices)}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯æ ·æœ¬
    num_show = min(max_errors, len(error_indices))
    print(f"æ˜¾ç¤ºå‰ {num_show} ä¸ªé”™è¯¯æ ·æœ¬:")
    
    for i, idx in enumerate(error_indices[:num_show]):
        print(f"\né”™è¯¯æ ·æœ¬ {i+1} (ç´¢å¼• {idx}):")
        print(f"  çœŸå®æ ‡ç­¾: {y_true[idx]}")
        print(f"  é¢„æµ‹æ ‡ç­¾: {y_pred[idx]}")
        
        if X is not None and feature_names is not None:
            print(f"  ç‰¹å¾å€¼:")
            for j, feature_name in enumerate(feature_names):
                if j < X.shape[1]:
                    print(f"    {feature_name}: {X[idx, j]:.4f}")
    
    # é”™è¯¯ç±»å‹ç»Ÿè®¡
    print(f"\nğŸ“Š é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    for true_label in np.unique(y_true):
        for pred_label in np.unique(y_pred):
            if true_label != pred_label:
                count = np.sum((y_true == true_label) & (y_pred == pred_label))
                if count > 0:
                    print(f"  çœŸå®:{true_label} -> é¢„æµ‹:{pred_label}: {count} ä¸ªæ ·æœ¬")


def plot_metrics_comparison(metrics_dict, save_path=None, figsize=(12, 8)):
    """ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„æŒ‡æ ‡å¯¹æ¯”å›¾"""
    try:
        # è·å–æ ‡ç­¾é…ç½®
        labels = get_chinese_labels()
        
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
        
        # æå–æŒ‡æ ‡æ•°æ®
        models = list(metrics_dict.keys())
        accuracy = [metrics_dict[model]['accuracy'] for model in models]
        precision = [metrics_dict[model]['precision_weighted'] for model in models]
        recall = [metrics_dict[model]['recall_weighted'] for model in models]
        f1 = [metrics_dict[model]['f1_weighted'] for model in models]
        
        # ç»˜åˆ¶å„ä¸ªæŒ‡æ ‡
        axes[0, 0].bar(models, accuracy, alpha=0.7, color='skyblue')
        axes[0, 0].set_title('Accuracy')
        axes[0, 0].set_ylim(0, 1)
        
        axes[0, 1].bar(models, precision, alpha=0.7, color='lightgreen')
        axes[0, 1].set_title('Precision')
        axes[0, 1].set_ylim(0, 1)
        
        axes[1, 0].bar(models, recall, alpha=0.7, color='salmon')
        axes[1, 0].set_title('Recall')
        axes[1, 0].set_ylim(0, 1)
        
        axes[1, 1].bar(models, f1, alpha=0.7, color='gold')
        axes[1, 1].set_title('F1-Score')
        axes[1, 1].set_ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for ax in axes.flat:
            for i, v in enumerate(ax.containers[0]):
                height = v.get_height()
                ax.text(v.get_x() + v.get_width()/2., height + 0.01,
                       f'{height:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        if save_path:
            save_figure_with_chinese_support(fig, save_path)
            
    except Exception as e:
        print(f"æŒ‡æ ‡å¯¹æ¯”å›¾ç»˜åˆ¶å¤±è´¥: {e}") 