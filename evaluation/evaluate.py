"""
ä¸»è¯„ä¼°è„šæœ¬
"""

import argparse
import torch
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from training.train_simple import SimplePETNet, PETDataset
from evaluation.metrics import print_classification_report, plot_roc_curve, plot_precision_recall_curve, analyze_prediction_errors


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='PET-GNNæ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model_path', type=str, default='experiments/best_model.pth',
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--test_data', type=str, default='data/processed/test_data.csv',
                       help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='evaluation_results',
                       help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--save_plots', action='store_true',
                       help='æ˜¯å¦ä¿å­˜å¯è§†åŒ–å›¾è¡¨')
    
    return parser.parse_args()


def load_model_for_evaluation(model_path, device='cpu'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨äºè¯„ä¼°
    
    Args:
        model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        config: æ¨¡å‹é…ç½®
    """
    try:
        checkpoint = torch.load(model_path, map_location=device)
        
        # è·å–é…ç½®ä¿¡æ¯
        config = checkpoint.get('config', {})
        
        # åˆ›å»ºæ¨¡å‹
        model = SimplePETNet(
            input_dim=10,
            hidden_dims=config.get('model', {}).get('hidden_dims', [64, 32, 16]),
            num_classes=2,
            dropout=config.get('model', {}).get('dropout', 0.1)
        )
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        print(f"   è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'Unknown')}")
        print(f"   æœ€ä½³æŒ‡æ ‡: {checkpoint.get('metrics', {})}")
        
        return model, config
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


def evaluate_model(model, test_loader, device='cpu'):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    all_preds = []
    all_targets = []
    all_probs = []
    
    print("ğŸ”„ æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")
    
    with torch.no_grad():
        for batch_idx, (data, targets) in enumerate(test_loader):
            data, targets = data.to(device), targets.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(data)
            probs = torch.softmax(outputs, dim=1)
            preds = outputs.argmax(dim=1)
            
            # æ”¶é›†ç»“æœ
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    y_true = np.array(all_targets)
    y_pred = np.array(all_preds)
    y_prob = np.array(all_probs)
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼Œå…±å¤„ç† {len(y_true)} ä¸ªæ ·æœ¬")
    
    return {
        'y_true': y_true,
        'y_pred': y_pred, 
        'y_prob': y_prob
    }


def create_test_data_loader(test_data_path, batch_size=32):
    """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    try:
        test_dataset = PETDataset(test_data_path)
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0
        )
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(test_dataset)} ä¸ªæ ·æœ¬")
        return test_loader
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
        # å¦‚æœæµ‹è¯•æ•°æ®ä¸å­˜åœ¨ï¼Œä½¿ç”¨éªŒè¯æ•°æ®ä»£æ›¿
        print("å°è¯•ä½¿ç”¨éªŒè¯æ•°æ®è¿›è¡Œè¯„ä¼°...")
        try:
            val_dataset = PETDataset('data/processed/val_data.csv')
            val_loader = torch.utils.data.DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0
            )
            print(f"âœ… ä½¿ç”¨éªŒè¯æ•°æ®è¯„ä¼°ï¼Œå…± {len(val_dataset)} ä¸ªæ ·æœ¬")
            return val_loader
        except Exception as e2:
            print(f"âŒ éªŒè¯æ•°æ®ä¹ŸåŠ è½½å¤±è´¥: {e2}")
            raise


def main():
    # è§£æå‚æ•°
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹PET-GNNæ¨¡å‹è¯„ä¼°")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model, config = load_model_for_evaluation(args.model_path, device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_loader = create_test_data_loader(args.test_data, batch_size=32)
    
    # æ¨¡å‹è¯„ä¼°
    results = evaluate_model(model, test_loader, device)
    
    # æå–ç»“æœ
    y_true = results['y_true']
    y_pred = results['y_pred'] 
    y_prob = results['y_prob']
    
    # æ‰“å°è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
    class_names = ['æœ‰æ•ˆäº‹ä»¶', 'ä½è€¦åˆäº‹ä»¶']
    report_path = output_dir / 'classification_report.txt' if args.save_plots else None
    metrics = print_classification_report(y_true, y_pred, class_names, report_path)
    
    # é”™è¯¯æ ·æœ¬åˆ†æ
    feature_names = ['pos_i_x', 'pos_i_y', 'pos_i_z', 'pos_j_x', 'pos_j_y', 'pos_j_z', 
                    'E_i', 'E_j', 'distance', 'energy_diff']
    analyze_prediction_errors(y_true, y_pred, max_errors=5)
    
    # ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
    if args.save_plots:
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # ROCæ›²çº¿
        roc_path = output_dir / 'roc_curve.png'
        plot_roc_curve(y_true, y_prob, save_path=roc_path)
        
        # PRæ›²çº¿
        pr_path = output_dir / 'precision_recall_curve.png'
        plot_precision_recall_curve(y_true, y_prob, save_path=pr_path)
        
        print(f"ğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    results_df = pd.DataFrame({
        'true_label': y_true,
        'predicted_label': y_pred,
        'prob_class_0': y_prob[:, 0],
        'prob_class_1': y_prob[:, 1],
        'correct': y_true == y_pred
    })
    
    results_path = output_dir / 'prediction_results.csv'
    results_df.to_csv(results_path, index=False)
    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“ˆ è¯„ä¼°æ€»ç»“:")
    print(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {metrics['f1_weighted']:.4f}")
    if 'roc_auc' in metrics:
        print(f"  AUC:   {metrics['roc_auc']:.4f}")
    print("=" * 60)


if __name__ == '__main__':
    main() 