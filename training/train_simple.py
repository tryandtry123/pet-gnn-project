"""
PET-GNNç®€åŒ–è®­ç»ƒè„šæœ¬ (é¿å…å¤æ‚ä¾èµ–)
ä½¿ç”¨ç®€å•ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒæ¼”ç¤º
"""

import argparse
import yaml
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from pathlib import Path
import sys
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))


class SimplePETNet(nn.Module):
    """ç®€åŒ–çš„PETç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, input_dim=10, hidden_dims=[64, 32, 16], num_classes=2, dropout=0.1):
        super(SimplePETNet, self).__init__()
        
        self.input_dim = input_dim
        self.num_classes = num_classes
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, num_classes))
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        return self.network(x)


class PETDataset(torch.utils.data.Dataset):
    """ç®€åŒ–çš„PETæ•°æ®é›†"""
    
    def __init__(self, csv_file, feature_cols=None):
        self.data = pd.read_csv(csv_file)
        
        if feature_cols is None:
            # é€‰æ‹©ä¸»è¦ç‰¹å¾ï¼ˆæ’é™¤æ ‡ç­¾ï¼‰
            feature_cols = ['pos_i_x', 'pos_i_y', 'pos_i_z', 'pos_j_x', 'pos_j_y', 'pos_j_z', 
                          'E_i', 'E_j', 'distance', 'energy_diff']
        
        self.features = self.data[feature_cols].values.astype(np.float32)
        self.labels = self.data['label'].values.astype(np.int64)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)
        
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='PET-GNNç®€åŒ–è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config/default.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--debug', action='store_true',
                       help='è°ƒè¯•æ¨¡å¼')
    
    return parser.parse_args()


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return create_default_config()


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'training': {
            'batch_size': 32,
            'learning_rate': 0.001,
            'weight_decay': 1e-4,
            'epochs': 50,
            'optimizer': 'Adam',
            'scheduler': 'StepLR',
            'step_size': 30,
            'gamma': 0.1,
            'early_stopping': {'patience': 10}
        },
        'model': {
            'hidden_dims': [64, 32, 16],
            'dropout': 0.1
        },
        'experiment': {
            'save_dir': 'experiments'
        }
    }


def setup_device():
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device('cpu')
        print("ä½¿ç”¨CPU")
    return device


def create_data_loaders(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    try:
        train_dataset = PETDataset('data/processed/train_data.csv')
        val_dataset = PETDataset('data/processed/val_data.csv')
        
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config['training']['batch_size'],
            shuffle=True,
            num_workers=0
        )
        
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=config['training']['batch_size'],
            shuffle=False,
            num_workers=0
        )
        
        print(f"æ•°æ®åŠ è½½æˆåŠŸ: è®­ç»ƒé›†{len(train_dataset)}, éªŒè¯é›†{len(val_dataset)}")
        return train_loader, val_loader
        
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡Œ create_test_data.py ç”Ÿæˆæ•°æ®")
        raise


def create_model(config):
    """åˆ›å»ºæ¨¡å‹"""
    input_dim = 10  # ç‰¹å¾ç»´åº¦
    hidden_dims = config.get('model', {}).get('hidden_dims', [64, 32, 16])
    dropout = config.get('model', {}).get('dropout', 0.1)
    
    model = SimplePETNet(
        input_dim=input_dim,
        hidden_dims=hidden_dims,
        num_classes=2,
        dropout=dropout
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {total_params:,}")
    
    return model


def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
    optimizer_name = config['training']['optimizer']
    lr = config['training']['learning_rate']
    weight_decay = config['training']['weight_decay']
    
    if optimizer_name == 'Adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'SGD':
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)
    else:
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, 
        step_size=config['training'].get('step_size', 30),
        gamma=config['training'].get('gamma', 0.1)
    )
    
    return optimizer, scheduler


def train_epoch(model, train_loader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc='è®­ç»ƒ')
    for batch_idx, (data, targets) in enumerate(pbar):
        data, targets = data.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        pred = outputs.argmax(dim=1)
        correct += pred.eq(targets).sum().item()
        total += targets.size(0)
        
        # æ›´æ–°è¿›åº¦æ¡
        accuracy = 100. * correct / total
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{accuracy:.2f}%'
        })
    
    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy


def validate_epoch(model, val_loader, criterion, device):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, targets in tqdm(val_loader, desc='éªŒè¯'):
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item()
            pred = outputs.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    avg_loss = total_loss / len(val_loader)
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_targets, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_targets, all_preds, average='weighted', zero_division=0)
    
    return avg_loss, {
        'accuracy': accuracy,
        'precision': precision, 
        'recall': recall,
        'f1': f1
    }


def save_checkpoint(model, optimizer, epoch, metrics, config, save_dir, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config
    }
    
    # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
    checkpoint_path = save_dir / 'latest_checkpoint.pth'
    torch.save(checkpoint, checkpoint_path)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if is_best:
        best_path = save_dir / 'best_model.pth'
        torch.save(checkpoint, best_path)
        print(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")


def main():
    # è§£æå‚æ•°
    args = parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®æ—¥å¿—
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹PET-GNNç®€åŒ–è®­ç»ƒ")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_data_loaders(config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)
    model = model.to(device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer, scheduler = create_optimizer(model, config)
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.to(device)
    
    # è®­ç»ƒè®¾ç½®
    epochs = config['training']['epochs']
    best_f1 = 0
    patience = config['training']['early_stopping']['patience']
    patience_counter = 0
    save_dir = config['experiment']['save_dir']
    
    print(f"è®­ç»ƒè®¾ç½®:")
    print(f"  - è®­ç»ƒè½®æ•°: {epochs}")
    print(f"  - æ‰¹å¤§å°: {config['training']['batch_size']}")
    print(f"  - å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"  - æ—©åœè€å¿ƒ: {patience}")
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    if args.resume:
        try:
            checkpoint = torch.load(args.resume, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_f1 = checkpoint['metrics']['f1']
            print(f"ä»epoch {start_epoch}æ¢å¤è®­ç»ƒ")
        except Exception as e:
            print(f"æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
    
    # è®­ç»ƒå¾ªç¯
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯")
    for epoch in range(start_epoch, epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        print("-" * 40)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # éªŒè¯
        val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•ç»“æœ
        print(f"è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_metrics['accuracy']*100:.2f}%")
        print(f"      - Precision: {val_metrics['precision']:.4f}")
        print(f"      - Recall: {val_metrics['recall']:.4f}")
        print(f"      - F1: {val_metrics['f1']:.4f}")
        print(f"å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        is_best = val_metrics['f1'] > best_f1
        if is_best:
            best_f1 = val_metrics['f1']
            patience_counter = 0
            print(f"ğŸ‰ æ–°çš„æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
        else:
            patience_counter += 1
        
        save_checkpoint(model, optimizer, epoch, val_metrics, config, save_dir, is_best)
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"æ—©åœè§¦å‘ï¼Œå·²è¿ç»­{patience}è½®æ— æ”¹å–„")
            break
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
    print("=" * 60)


if __name__ == '__main__':
    main() 