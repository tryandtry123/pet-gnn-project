"""
é«˜æ€§èƒ½å›¾æ„å»ºå™¨ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ— å¤–éƒ¨ä¾èµ–ï¼‰
å¤§å¹…æå‡å›¾æ•°æ®ç”Ÿæˆé€Ÿåº¦ï¼Œå‡å°‘è®¡ç®—æ—¶é—´
"""

import numpy as np
import pandas as pd
import torch
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cdist
from scipy.sparse import csr_matrix
import networkx as nx
from pathlib import Path
import pickle
import logging
import time

class FastGraphBuilder:
    """
    é«˜æ€§èƒ½PETå›¾æ„å»ºå™¨
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. å‘é‡åŒ–è®¡ç®—æ›¿ä»£å¾ªç¯
    2. é¢„è®¡ç®—å’Œç¼“å­˜é‡ç”¨
    3. ç¨€ç–çŸ©é˜µå­˜å‚¨
    4. åˆ†æ‰¹å¤„ç†å¤§æ•°æ®
    5. ç®—æ³•ç®€åŒ–ä¼˜åŒ–
    """
    
    def __init__(self, config):
        self.config = config
        self.graph_config = config['graph']
        
        # å›¾æ„å»ºå‚æ•°
        self.k_neighbors = self.graph_config['k_neighbors']
        self.distance_threshold = self.graph_config['distance_threshold']
        self.coupling_threshold = self.graph_config['coupling_threshold']
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.batch_size = 1000  # åˆ†æ‰¹å¤„ç†å¤§å°
        self.use_sparse = True  # ä½¿ç”¨ç¨€ç–çŸ©é˜µ
        self.cache_positions = True  # ç¼“å­˜ä½ç½®æŸ¥æ‰¾
        
        # ç‰¹å¾æ ‡å‡†åŒ–å™¨
        self.node_scaler = StandardScaler()
        self.edge_scaler = StandardScaler()
        
        # ç¼“å­˜å˜é‡
        self.detector_positions = None
        self.position_to_idx = None  # ä½ç½®åˆ°ç´¢å¼•çš„æ˜ å°„
        self.adjacency_matrix = None
        
        logging.info(f"é«˜æ€§èƒ½å›¾æ„å»ºå™¨åˆå§‹åŒ–: k={self.k_neighbors}, "
                    f"distance_threshold={self.distance_threshold}, "
                    f"batch_size={self.batch_size}")
    
    def build_detector_graph(self, events_data):
        """
        é«˜æ€§èƒ½å›¾æ„å»ºä¸»å‡½æ•°
        """
        start_time = time.time()
        logging.info("ğŸš€ å¼€å§‹é«˜æ€§èƒ½å›¾æ„å»º...")
        
        # 1. å¿«é€Ÿæå–æ¢æµ‹å™¨ä½ç½®ï¼ˆå‘é‡åŒ–ï¼‰
        detector_positions = self._extract_detector_positions_fast(events_data)
        
        # 2. é«˜æ€§èƒ½é‚»æ¥çŸ©é˜µæ„å»º
        adjacency_matrix, edge_weights = self._build_adjacency_matrix_fast(detector_positions)
        
        # 3. å¿«é€ŸèŠ‚ç‚¹ç‰¹å¾æå–
        node_features = self._extract_node_features_fast(detector_positions, events_data)
        
        # 4. å¿«é€Ÿè¾¹ç‰¹å¾æå–ï¼ˆæœ€å¤§ä¼˜åŒ–ï¼‰
        edge_features = self._extract_edge_features_fast(adjacency_matrix, detector_positions, events_data)
        
        # 5. æ„å»ºå›¾æ•°æ®
        graph_data = {
            'detector_positions': detector_positions,
            'adjacency_matrix': adjacency_matrix,
            'edge_weights': edge_weights,
            'node_features': node_features,
            'edge_features': edge_features,
            'num_nodes': len(detector_positions),
            'num_edges': np.sum(adjacency_matrix > 0) if not self.use_sparse else adjacency_matrix.nnz
        }
        
        # 6. PyTorchæ ¼å¼è½¬æ¢
        torch_data = self._convert_to_torch_format_fast(graph_data)
        graph_data.update(torch_data)
        
        total_time = time.time() - start_time
        logging.info(f"âœ… é«˜æ€§èƒ½å›¾æ„å»ºå®Œæˆ: {graph_data['num_nodes']} èŠ‚ç‚¹, "
                    f"{graph_data['num_edges']} è¾¹, è€—æ—¶ {total_time:.2f}s")
        
        return graph_data
    
    def _extract_detector_positions_fast(self, events_data):
        """å‘é‡åŒ–çš„æ¢æµ‹å™¨ä½ç½®æå–"""
        start_time = time.time()
        
        # å‘é‡åŒ–æ“ä½œï¼šä¸€æ¬¡æ€§æå–æ‰€æœ‰ä½ç½®
        pos_cols_i = ['pos_i_x', 'pos_i_y', 'pos_i_z']
        pos_cols_j = ['pos_j_x', 'pos_j_y', 'pos_j_z']
        
        pos_i = events_data[pos_cols_i].values
        pos_j = events_data[pos_cols_j].values
        
        # ä½¿ç”¨numpyçš„é«˜æ•ˆå»é‡
        all_positions = np.vstack([pos_i, pos_j])
        unique_positions = np.unique(all_positions, axis=0)
        
        # æ„å»ºä½ç½®åˆ°ç´¢å¼•çš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸ï¼ˆç”¨äºåç»­åŠ é€Ÿï¼‰
        if self.cache_positions:
            self.position_to_idx = {}
            for idx, pos in enumerate(unique_positions):
                key = tuple(pos.round(6))  # é¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
                self.position_to_idx[key] = idx
        
        elapsed = time.time() - start_time
        logging.info(f"å‘ç° {len(unique_positions)} ä¸ªå”¯ä¸€æ¢æµ‹å™¨ä½ç½® (è€—æ—¶ {elapsed:.2f}s)")
        
        return unique_positions
    
    def _build_adjacency_matrix_fast(self, detector_positions):
        """é«˜æ€§èƒ½é‚»æ¥çŸ©é˜µæ„å»º"""
        start_time = time.time()
        num_detectors = len(detector_positions)
        
        if self.use_sparse:
            # ä½¿ç”¨ç¨€ç–çŸ©é˜µèŠ‚çœå†…å­˜
            rows, cols, data = [], [], []
        else:
            adjacency_matrix = np.zeros((num_detectors, num_detectors), dtype=np.float32)
        
        # ä¼˜åŒ–çš„k-è¿‘é‚»è®¡ç®—
        if self.k_neighbors > 0:
            knn = NearestNeighbors(n_neighbors=min(self.k_neighbors + 1, num_detectors), 
                                 metric='euclidean', algorithm='kd_tree')
            knn.fit(detector_positions)
            distances, indices = knn.kneighbors(detector_positions)
            
            # å‘é‡åŒ–æƒé‡è®¡ç®—
            weights = np.exp(-distances / 10.0)
            
            for i in range(num_detectors):
                for j in range(1, min(self.k_neighbors + 1, len(indices[i]))):
                    neighbor_idx = indices[i, j]
                    weight = weights[i, j]
                    
                    if self.use_sparse:
                        rows.extend([i, neighbor_idx])
                        cols.extend([neighbor_idx, i])
                        data.extend([weight, weight])
                    else:
                        adjacency_matrix[i, neighbor_idx] = weight
                        adjacency_matrix[neighbor_idx, i] = weight
        
        # è·ç¦»é˜ˆå€¼è¿æ¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        if self.distance_threshold > 0:
            # åˆ†æ‰¹è®¡ç®—è·ç¦»çŸ©é˜µï¼Œé¿å…å†…å­˜æº¢å‡º
            for i in range(0, num_detectors, self.batch_size):
                end_i = min(i + self.batch_size, num_detectors)
                batch_positions = detector_positions[i:end_i]
                
                # è®¡ç®—å½“å‰æ‰¹æ¬¡åˆ°æ‰€æœ‰ç‚¹çš„è·ç¦»
                distance_batch = cdist(batch_positions, detector_positions, metric='euclidean')
                
                # æ‰¾åˆ°é˜ˆå€¼å†…çš„è¿æ¥
                mask = (distance_batch <= self.distance_threshold) & (distance_batch > 0)
                batch_rows, batch_cols = np.where(mask)
                
                # è°ƒæ•´ç´¢å¼•
                batch_rows += i
                
                # è®¡ç®—æƒé‡
                batch_weights = 1.0 / (1.0 + distance_batch[mask])
                
                if self.use_sparse:
                    rows.extend(batch_rows)
                    cols.extend(batch_cols)
                    data.extend(batch_weights)
                else:
                    adjacency_matrix[batch_rows, batch_cols] = np.maximum(
                        adjacency_matrix[batch_rows, batch_cols], batch_weights
                    )
        
        # æ„å»ºæœ€ç»ˆçŸ©é˜µ
        if self.use_sparse:
            adjacency_matrix = csr_matrix((data, (rows, cols)), 
                                        shape=(num_detectors, num_detectors))
            edge_weights = np.array(data)
        else:
            edge_weights = adjacency_matrix[adjacency_matrix > 0]
        
        density = len(data) / (num_detectors * num_detectors) if self.use_sparse else np.mean(adjacency_matrix > 0)
        elapsed = time.time() - start_time
        logging.info(f"é‚»æ¥çŸ©é˜µæ„å»ºå®Œæˆ: å¯†åº¦ {density:.3f} (è€—æ—¶ {elapsed:.2f}s)")
        
        return adjacency_matrix, edge_weights
    
    def _extract_node_features_fast(self, detector_positions, events_data):
        """å¿«é€ŸèŠ‚ç‚¹ç‰¹å¾æå–ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰"""
        start_time = time.time()
        num_detectors = len(detector_positions)
        
        # åŸºç¡€ç‰¹å¾ï¼šç©ºé—´åæ ‡
        node_features = detector_positions.copy()
        
        # é¢„åˆ†é…ç»Ÿè®¡ç‰¹å¾æ•°ç»„
        detector_stats = np.zeros((num_detectors, 4))  # [crystal_size, avg_energy, event_count, coupling_events]
        
        # å‘é‡åŒ–åŒ¹é…ï¼šé¢„è®¡ç®—æ‰€æœ‰ä½ç½®åŒ¹é…
        pos_i_data = events_data[['pos_i_x', 'pos_i_y', 'pos_i_z']].values
        pos_j_data = events_data[['pos_j_x', 'pos_j_y', 'pos_j_z']].values
        
        # ä¸ºæ¯ä¸ªæ¢æµ‹å™¨è¿›è¡Œå‘é‡åŒ–ç»Ÿè®¡
        for idx, detector_pos in enumerate(detector_positions):
            # å‘é‡åŒ–ä½ç½®åŒ¹é…ï¼ˆé¿å…å¾ªç¯ï¼‰
            pos_i_match = np.all(np.abs(pos_i_data - detector_pos) < 1e-6, axis=1)
            pos_j_match = np.all(np.abs(pos_j_data - detector_pos) < 1e-6, axis=1)
            
            # å‘é‡åŒ–ç»Ÿè®¡è®¡ç®—
            if np.any(pos_i_match) or np.any(pos_j_match):
                # èƒ½é‡ç»Ÿè®¡
                energies = []
                if np.any(pos_i_match):
                    energies.extend(events_data.loc[pos_i_match, 'E_i'].values)
                if np.any(pos_j_match):
                    energies.extend(events_data.loc[pos_j_match, 'E_j'].values)
                
                avg_energy = np.mean(energies) if energies else 511.0
                event_count = np.sum(pos_i_match) + np.sum(pos_j_match)
                
                # ä½è€¦åˆäº‹ä»¶ç»Ÿè®¡
                coupling_i = np.sum(events_data.loc[pos_i_match, 'label'] == 1) if np.any(pos_i_match) else 0
                coupling_j = np.sum(events_data.loc[pos_j_match, 'label'] == 1) if np.any(pos_j_match) else 0
                coupling_events = coupling_i + coupling_j
                
                detector_stats[idx] = [
                    2.0 + 0.1 * np.sin(detector_pos[0] * 0.1),  # crystal_size
                    avg_energy,
                    event_count,
                    coupling_events
                ]
            else:
                detector_stats[idx] = [2.0 + 0.1 * np.sin(detector_pos[0] * 0.1), 511.0, 0, 0]
        
        # åˆå¹¶ç‰¹å¾
        node_features = np.column_stack([node_features, detector_stats])
        
        # æ ‡å‡†åŒ–
        node_features = self.node_scaler.fit_transform(node_features)
        
        elapsed = time.time() - start_time
        logging.info(f"èŠ‚ç‚¹ç‰¹å¾æå–å®Œæˆ: å½¢çŠ¶ {node_features.shape} (è€—æ—¶ {elapsed:.2f}s)")
        
        return node_features.astype(np.float32)
    
    def _extract_edge_features_fast(self, adjacency_matrix, detector_positions, events_data):
        """è¶…é«˜é€Ÿè¾¹ç‰¹å¾æå–ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰"""
        start_time = time.time()
        
        # è·å–è¾¹ç´¢å¼•
        if self.use_sparse:
            edge_indices = adjacency_matrix.nonzero()
            edge_weights_values = adjacency_matrix.data
        else:
            edge_indices = np.where(adjacency_matrix > 0)
            edge_weights_values = adjacency_matrix[edge_indices]
        
        num_edges = len(edge_indices[0])
        
        if num_edges == 0:
            logging.info("æ— è¾¹ï¼Œè¿”å›ç©ºç‰¹å¾")
            return np.array([])
        
        # å‘é‡åŒ–è·ç¦»è®¡ç®—
        pos_i = detector_positions[edge_indices[0]]
        pos_j = detector_positions[edge_indices[1]]
        distances = np.linalg.norm(pos_i - pos_j, axis=1)
        
        # ä¼˜åŒ–çš„å…±ç°è®¡ç®—ï¼ˆåŸºäºè·ç¦»æ¨¡å‹ï¼‰
        cooccurrences = np.exp(-distances / 50.0)  # ç®€åŒ–æ¨¡å‹ï¼Œé¿å…å¤æ‚è®¡ç®—
        
        # ç»„åˆè¾¹ç‰¹å¾
        if len(edge_weights_values) == num_edges:
            edge_features = np.column_stack([distances, edge_weights_values, cooccurrences])
        else:
            edge_features = np.column_stack([distances, np.ones(num_edges), cooccurrences])
        
        # æ ‡å‡†åŒ–
        if len(edge_features) > 0:
            edge_features = self.edge_scaler.fit_transform(edge_features)
        
        elapsed = time.time() - start_time
        logging.info(f"è¾¹ç‰¹å¾æå–å®Œæˆ: {num_edges} æ¡è¾¹ (è€—æ—¶ {elapsed:.2f}s)")
        
        return edge_features.astype(np.float32)
    
    def _convert_to_torch_format_fast(self, graph_data):
        """å¿«é€ŸPyTorchæ ¼å¼è½¬æ¢"""
        # è·å–è¾¹ç´¢å¼•
        if self.use_sparse:
            edge_index = torch.tensor(np.vstack(graph_data['adjacency_matrix'].nonzero()), dtype=torch.long)
        else:
            edge_indices = np.where(graph_data['adjacency_matrix'] > 0)
            edge_index = torch.tensor(np.vstack(edge_indices), dtype=torch.long)
        
        torch_data = {
            'x': torch.tensor(graph_data['node_features'], dtype=torch.float),
            'edge_index': edge_index,
            'edge_attr': torch.tensor(graph_data['edge_features'], dtype=torch.float) if len(graph_data['edge_features']) > 0 else None,
            'edge_weight': torch.tensor(graph_data['edge_weights'], dtype=torch.float)
        }
        
        return torch_data
    
    def save_graph_cache(self, graph_data, cache_path):
        """ä¿å­˜å›¾æ•°æ®ç¼“å­˜"""
        cache_path = Path(cache_path)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(cache_path, 'wb') as f:
            pickle.dump(graph_data, f)
        
        logging.info(f"å›¾æ•°æ®ç¼“å­˜å·²ä¿å­˜: {cache_path}")
    
    def load_graph_cache(self, cache_path):
        """åŠ è½½å›¾æ•°æ®ç¼“å­˜"""
        try:
            with open(cache_path, 'rb') as f:
                graph_data = pickle.load(f)
            logging.info(f"å›¾æ•°æ®ç¼“å­˜å·²åŠ è½½: {cache_path}")
            return graph_data
        except FileNotFoundError:
            logging.info(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return None 