"""
ç®€åŒ–ç‰ˆPET-GNNè®­ç»ƒè„šæœ¬
ä½¿ç”¨åŸºæœ¬PyTorchåŠŸèƒ½ï¼Œé¿å…å¤æ‚ä¾èµ–

ä¸»è¦åŠŸèƒ½ï¼š
1. å®šä¹‰SimplePETNetç¥ç»ç½‘ç»œæ¨¡å‹
2. åˆ›å»ºPETæ•°æ®é›†åŠ è½½å™¨
3. å®ç°è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
4. æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹
5. ä¿å­˜æœ€ä½³æ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•é›†è¯„ä¼°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from pathlib import Path
import logging

# è®¾ç½®æ—¥å¿—æ ¼å¼ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œç›‘æ§è®­ç»ƒè¿‡ç¨‹
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class SimplePETNet(nn.Module):
    """
    ç®€åŒ–çš„PETç¥ç»ç½‘ç»œæ¨¡å‹
    
    åŠŸèƒ½ï¼šä½¿ç”¨å…¨è¿æ¥å±‚å¤„ç†PETäº‹ä»¶ç‰¹å¾ï¼Œåˆ¤æ–­äº‹ä»¶æ˜¯å¦ä¸ºä½è€¦åˆäº‹ä»¶
    
    ç½‘ç»œç»“æ„ï¼š
    - è¾“å…¥å±‚ï¼šæ¥æ”¶PETäº‹ä»¶çš„å¤šç»´ç‰¹å¾
    - å¤šä¸ªéšè—å±‚ï¼šæ¯å±‚åŒ…å«çº¿æ€§å˜æ¢ã€ReLUæ¿€æ´»ã€æ‰¹å½’ä¸€åŒ–ã€Dropout
    - è¾“å‡ºå±‚ï¼šè¾“å‡º2ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼ˆæœ‰æ•ˆäº‹ä»¶/ä½è€¦åˆäº‹ä»¶ï¼‰
    
    Args:
        input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆé»˜è®¤10ä¸ªç‰¹å¾ï¼‰
        hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼ˆé»˜è®¤[64, 32, 16]é€å±‚é€’å‡ï¼‰
        num_classes: åˆ†ç±»ç±»åˆ«æ•°ï¼ˆé»˜è®¤2ï¼šå¥½äº‹ä»¶/åäº‹ä»¶ï¼‰
        dropout: Dropoutæ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆé»˜è®¤0.1ï¼‰
    """
    
    def __init__(self, input_dim=10, hidden_dims=[64, 32, 16], num_classes=2, dropout=0.1):
        super(SimplePETNet, self).__init__()
        
        # ä¿å­˜æ¨¡å‹é…ç½®å‚æ•°
        self.input_dim = input_dim
        self.num_classes = num_classes
        
        # åŠ¨æ€æ„å»ºç½‘ç»œå±‚åˆ—è¡¨
        layers = []
        prev_dim = input_dim  # å‰ä¸€å±‚çš„è¾“å‡ºç»´åº¦
        
        # å¾ªç¯åˆ›å»ºéšè—å±‚
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),    # çº¿æ€§å˜æ¢å±‚
                nn.ReLU(),                          # ReLUæ¿€æ´»å‡½æ•°
                nn.BatchNorm1d(hidden_dim),         # æ‰¹å½’ä¸€åŒ–ï¼ŒåŠ é€Ÿè®­ç»ƒ
                nn.Dropout(dropout)                 # Dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            ])
            prev_dim = hidden_dim  # æ›´æ–°ç»´åº¦ä¸ºå½“å‰å±‚è¾“å‡º
        
        # æ·»åŠ è¾“å‡ºå±‚ï¼ˆä¸éœ€è¦æ¿€æ´»å‡½æ•°ï¼Œåé¢ä¼šç”¨softmaxï¼‰
        layers.append(nn.Linear(prev_dim, num_classes))
        
        # å°†æ‰€æœ‰å±‚ç»„åˆæˆä¸€ä¸ªé¡ºåºç½‘ç»œ
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """
        åˆå§‹åŒ–ç½‘ç»œæƒé‡
        ä½¿ç”¨Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§
        """
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # å¯¹çº¿æ€§å±‚ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    # åç½®é¡¹åˆå§‹åŒ–ä¸º0
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        
        Args:
            x: è¾“å…¥ç‰¹å¾å¼ é‡ [batch_size, input_dim]
            
        Returns:
            è¾“å‡ºå¼ é‡ [batch_size, num_classes]
        """
        return self.network(x)


class PETDataset(torch.utils.data.Dataset):
    """
    ç®€åŒ–çš„PETæ•°æ®é›†ç±»
    
    åŠŸèƒ½ï¼š
    1. ä»CSVæ–‡ä»¶åŠ è½½PETäº‹ä»¶æ•°æ®
    2. æå–æŒ‡å®šçš„ç‰¹å¾åˆ—
    3. å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
    4. æä¾›PyTorch DataLoaderå…¼å®¹çš„æ¥å£
    
    æ•°æ®æ ¼å¼ï¼š
    - ç‰¹å¾ï¼šæ¢æµ‹å™¨ä½ç½®ã€èƒ½é‡ã€è·ç¦»ç­‰10ä¸ªç‰¹å¾
    - æ ‡ç­¾ï¼š0=æœ‰æ•ˆäº‹ä»¶ï¼Œ1=ä½è€¦åˆäº‹ä»¶
    """
    
    def __init__(self, csv_file, feature_cols=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            csv_file: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
            feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç‰¹å¾
        """
        # è¯»å–CSVæ–‡ä»¶
        self.data = pd.read_csv(csv_file)
        
        if feature_cols is None:
            # ä½¿ç”¨é»˜è®¤çš„ä¸»è¦ç‰¹å¾ï¼ˆæ’é™¤æ ‡ç­¾åˆ—ï¼‰
            feature_cols = [
                'pos_i_x', 'pos_i_y', 'pos_i_z',  # æ¢æµ‹å™¨içš„3Dåæ ‡
                'pos_j_x', 'pos_j_y', 'pos_j_z',  # æ¢æµ‹å™¨jçš„3Dåæ ‡  
                'E_i', 'E_j',                      # ä¸¤ä¸ªæ¢æµ‹å™¨çš„èƒ½é‡
                'distance', 'energy_diff'          # è®¡ç®—å¾—å‡ºçš„è·ç¦»å’Œèƒ½é‡å·®
            ]
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        self.features = self.data[feature_cols].values.astype(np.float32)
        self.labels = self.data['label'].values.astype(np.int64)
        
        # å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰
        # è¿™æ ·å¯ä»¥è®©ä¸åŒå°ºåº¦çš„ç‰¹å¾åœ¨åŒä¸€æ°´å¹³ï¼Œæœ‰åŠ©äºè®­ç»ƒ
        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)
        
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.features)
    
    def __getitem__(self, idx):
        """
        è·å–æŒ‡å®šç´¢å¼•çš„æ•°æ®é¡¹
        
        Args:
            idx: æ•°æ®ç´¢å¼•
            
        Returns:
            (features, label): ç‰¹å¾å¼ é‡å’Œæ ‡ç­¾å¼ é‡
        """
        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])


def train_epoch(model, train_loader, optimizer, criterion, device):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ˆå®Œæ•´éå†ä¸€éè®­ç»ƒæ•°æ®ï¼‰
    
    Args:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼‰
        criterion: æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰
        device: è®¡ç®—è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: è®­ç»ƒå‡†ç¡®ç‡
    """
    model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutå’Œbatch normï¼‰
    total_loss = 0  # ç´¯è®¡æŸå¤±
    correct = 0     # æ­£ç¡®é¢„æµ‹æ•°é‡
    total = 0       # æ€»æ ·æœ¬æ•°é‡
    
    # éå†æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡
    for batch_idx, (data, target) in enumerate(train_loader):
        # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
        data, target = data.to(device), target.to(device)
        
        # æ¢¯åº¦æ¸…é›¶ï¼ˆPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼‰
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ¨¡å‹è¾“å‡º
        output = model(data)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        loss.backward()
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        optimizer.step()
        
        # ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
        total_loss += loss.item()
        pred = output.argmax(dim=1)  # è·å–é¢„æµ‹ç±»åˆ«
        correct += pred.eq(target).sum().item()  # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹æ•°é‡
        total += target.size(0)  # ç´¯è®¡æ ·æœ¬æ•°é‡
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy


def validate_epoch(model, val_loader, criterion, device):
    """
    éªŒè¯ä¸€ä¸ªepochï¼ˆåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼‰
    
    Args:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        avg_loss: å¹³å‡æŸå¤±
        accuracy: å‡†ç¡®ç‡
        precision: ç²¾ç¡®ç‡
        recall: å¬å›ç‡
        f1: F1åˆ†æ•°
    """
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutå’Œbatch normï¼‰
    total_loss = 0
    all_preds = []    # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_targets = []  # å­˜å‚¨æ‰€æœ‰çœŸå®æ ‡ç­¾
    
    # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—æ—¶é—´
    with torch.no_grad():
        for data, target in val_loader:
            # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            data, target = data.to(device), target.to(device)
            
            # å‰å‘ä¼ æ’­
            output = model(data)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output, target)
            
            # ç´¯è®¡æŸå¤±
            total_loss += loss.item()
            
            # è·å–é¢„æµ‹ç»“æœ
            pred = output.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / len(val_loader)
    
    # ä½¿ç”¨sklearnè®¡ç®—è¯¦ç»†çš„åˆ†ç±»æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_targets, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_targets, all_preds, average='weighted', zero_division=0)
    
    return avg_loss, accuracy, precision, recall, f1


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹ï¼š
    1. æ•°æ®å‡†å¤‡å’ŒåŠ è½½
    2. æ¨¡å‹åˆ›å»ºå’Œé…ç½®
    3. è®­ç»ƒå¾ªç¯æ‰§è¡Œ
    4. æœ€ä½³æ¨¡å‹ä¿å­˜
    5. æµ‹è¯•é›†è¯„ä¼°
    """
    print("ğŸš€ å¼€å§‹PET-GNNæ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # 1. è®¾å¤‡é…ç½®
    # ä¼˜å…ˆä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 2. æ•°æ®æ–‡ä»¶æ£€æŸ¥
    train_file = 'data/processed/train_data.csv'
    val_file = 'data/processed/val_data.csv'
    test_file = 'data/processed/test_data.csv'
    
    # å¦‚æœè®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºæµ‹è¯•æ•°æ®
    if not Path(train_file).exists():
        print("âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œé‡æ–°åˆ›å»º...")
        from create_test_data import create_test_pet_data
        create_test_pet_data()
    
    # 3. æ•°æ®é›†åˆ›å»º
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    train_dataset = PETDataset(train_file)  # è®­ç»ƒé›†
    val_dataset = PETDataset(val_file)      # éªŒè¯é›†
    test_dataset = PETDataset(test_file)    # æµ‹è¯•é›†
    
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    # 4. æ•°æ®åŠ è½½å™¨åˆ›å»º
    batch_size = 32  # æ‰¹æ¬¡å¤§å°ï¼šæ¯æ¬¡å¤„ç†32ä¸ªæ ·æœ¬
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True    # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®é¡ºåº
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False   # éªŒè¯æ—¶ä¸æ‰“ä¹±
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False   # æµ‹è¯•æ—¶ä¸æ‰“ä¹±
    )
    
    # 5. æ¨¡å‹åˆ›å»º
    input_dim = train_dataset.features.shape[1]  # è‡ªåŠ¨è·å–ç‰¹å¾ç»´åº¦
    model = SimplePETNet(
        input_dim=input_dim,           # è¾“å…¥ç»´åº¦
        hidden_dims=[64, 32, 16],      # éšè—å±‚ç»´åº¦ï¼ˆé€å±‚é€’å‡ï¼‰
        num_classes=2                  # 2åˆ†ç±»ï¼šæœ‰æ•ˆäº‹ä»¶/ä½è€¦åˆäº‹ä»¶
    )
    model = model.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºæ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
    
    # 6. ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°é…ç½®
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=0.001,           # å­¦ä¹ ç‡
        weight_decay=1e-4   # L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    )
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±ï¼Œé€‚åˆå¤šåˆ†ç±»ä»»åŠ¡
    
    # 7. è®­ç»ƒå‚æ•°è®¾ç½®
    epochs = 50           # æœ€å¤§è®­ç»ƒè½®æ•°
    best_f1 = 0          # è®°å½•æœ€ä½³F1åˆ†æ•°
    patience = 10        # æ—©åœè€å¿ƒå€¼ï¼šè¿ç»­10è½®æ— æ”¹è¿›å°±åœæ­¢
    patience_counter = 0 # æ—©åœè®¡æ•°å™¨
    
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ ({epochs} epochs)")
    print("-" * 50)
    
    # 8. è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µï¼šæ¨¡å‹å­¦ä¹ è®­ç»ƒæ•°æ®
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # éªŒè¯é˜¶æ®µï¼šåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        val_loss, val_acc, val_precision, val_recall, val_f1 = validate_epoch(model, val_loader, criterion, device)
        
        # æ‰“å°å½“å‰è½®æ¬¡çš„è®­ç»ƒç»“æœ
        print(f"Epoch {epoch+1:2d}/{epochs} | "
              f"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | "
              f"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºF1åˆ†æ•°ï¼‰
        if val_f1 > best_f1:
            best_f1 = val_f1
            torch.save(model.state_dict(), 'best_model.pth')  # ä¿å­˜æ¨¡å‹æƒé‡
            patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹! F1: {val_f1:.4f}")
        else:
            patience_counter += 1  # å¢åŠ æ—©åœè®¡æ•°å™¨
        
        # æ—©åœæ£€æŸ¥ï¼šå¦‚æœè¿ç»­patienceè½®æ— æ”¹è¿›ï¼Œåœæ­¢è®­ç»ƒ
        if patience_counter >= patience:
            print(f"  â¹ï¸ æ—©åœè§¦å‘ (patience={patience})")
            break
    
    # 9. æµ‹è¯•é›†è¯„ä¼°
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•é›†è¯„ä¼°")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    model.load_state_dict(torch.load('best_model.pth'))
    test_loss, test_acc, test_precision, test_recall, test_f1 = validate_epoch(model, test_loader, criterion, device)
    
    # æ‰“å°æœ€ç»ˆæµ‹è¯•ç»“æœ
    print(f"æµ‹è¯•ç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {test_acc:.4f}")    # æ•´ä½“æ­£ç¡®ç‡
    print(f"  ç²¾ç¡®ç‡: {test_precision:.4f}")  # é¢„æµ‹ä¸ºæ­£ç±»ä¸­çœŸæ­£ä¸ºæ­£ç±»çš„æ¯”ä¾‹
    print(f"  å¬å›ç‡: {test_recall:.4f}")     # æ‰€æœ‰æ­£ç±»ä¸­è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹
    print(f"  F1åˆ†æ•°: {test_f1:.4f}")        # ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: best_model.pth")


# ç¨‹åºå…¥å£ç‚¹
if __name__ == "__main__":
    main() 